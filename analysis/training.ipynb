{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports, setup, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import Module, Linear, Dropout, ELU, Sequential, Sigmoid\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torch.optim import Adam\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utilities/')\n",
    "from utilities import Timer, ProgressBar\n",
    "\n",
    "sns.set()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect gpu availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: map returning a list\n",
    "def lmap(func, iterable):\n",
    "    return list(map(func, iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the Data\n",
    "### 1a. Preprocess/Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def preprocess_BERT(encoder, q_row):\n",
    "    '''Preprocess dataframe row for BERT\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder: callable, takes in text, returns encoded tokens, \n",
    "             should be provided by pre-trained model\n",
    "    \n",
    "    q_row  : dataframe row containing columns for question_title, \n",
    "             question_body, and answer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas Series of entries, each entry a list of length 512.\n",
    "    Entries: tokens, a mask, and positional embeddings.\n",
    "    '''\n",
    "    \n",
    "    # Set max length allowed by BERT model\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Get question title, body, and answer from dataframe row\n",
    "    question = q_row.question_title + q_row.question_body\n",
    "    answer   = q_row.answer\n",
    "    \n",
    "    # Encode question and answer without [CLS] and [SEP]\n",
    "    question_tok = encoder(question, add_special_tokens = False)\n",
    "    answer_tok   = encoder(answer, add_special_tokens = False)\n",
    "\n",
    "    # Truncate tokens to length MAX_LENGTH - 3 to account for special tokens\n",
    "    while len(question_tok + answer_tok) > (MAX_LENGTH - 3):\n",
    "        \n",
    "        # Pick the longest list, then pop last item in list\n",
    "        # Default to shortening answer if there is a tie\n",
    "        array_to_pop = max([answer_tok, question_tok], key = len)\n",
    "        array_to_pop.pop()\n",
    "    \n",
    "    # Get encodings for [CLS] and [SEP]\n",
    "    cls_token_encoded = encoder(['[CLS]'], add_special_tokens = False)\n",
    "    sep_token_encoded = encoder(['[SEP]'], add_special_tokens = False)\n",
    "    \n",
    "    # Combine question, answer, and special tokens\n",
    "    content_tok = cls_token_encoded + question_tok + \\\n",
    "                  sep_token_encoded + answer_tok   + \\\n",
    "                  sep_token_encoded\n",
    "    \n",
    "    # Create padding\n",
    "    padding_len = MAX_LENGTH - len(content_tok)\n",
    "    padding     = [0] * padding_len\n",
    "    \n",
    "    # Add padding\n",
    "    final_tok   = content_tok + padding\n",
    "    \n",
    "    # Compute segment_ids\n",
    "    segment_ids = [0] * (len(question_tok) + 2) + \\\n",
    "                  [1] * (len(answer_tok)   + 1) + \\\n",
    "                  padding\n",
    "    \n",
    "    # Compute the mask\n",
    "    mask        = [1] * len(content_tok) + padding\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tokens'      : final_tok,\n",
    "        'segment_ids' : segment_ids,\n",
    "        'mask'        : mask\n",
    "    })\n",
    "\n",
    "# Load in tokenizer for BERT base uncased\n",
    "BERT_base_uncased_tokenizer  = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased') \n",
    "\n",
    "# Curry preprocess function and partially apply it\n",
    "preprocess_BERT_base_uncased = functools.partial(preprocess_BERT, \n",
    "                                                 BERT_base_uncased_tokenizer.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Create a PyTorch Dataset from the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice', \n",
    "               'question_type_compare', 'question_type_consequence', \n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful', \n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, preprocessor, target_cols = None):\n",
    "    '''Create a dataset from a pandas dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df: Pandas dataframe with text columns available for the preprocessor \n",
    "        and containing the target columns\n",
    "        \n",
    "    preprocessor: callable taking a row of a dataframe and returning \n",
    "                  a Series containing the inputs as lists in each entry\n",
    "                  \n",
    "    target_cols: list of column names to use as the target.\n",
    "    If None, no labels are included\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    PyTorch Dataset (batched)\n",
    "    \n",
    "    '''\n",
    "    # Process the input data into a dataframe with 3 columns\n",
    "    processed_data = df.apply(preprocessor, axis = 'columns')\n",
    "\n",
    "    # Convert each of those three columns into a tensor\n",
    "    def convert_col_to_tensor(col):\n",
    "        # Convert each list entry to a tensor. Then stack them into one large tensor\n",
    "        col = lmap(lambda list_ : torch.tensor(list_, dtype = torch.long), col.tolist())\n",
    "        return torch.stack(col)\n",
    "\n",
    "    tokens      = convert_col_to_tensor(processed_data.tokens).to(device)\n",
    "    segment_ids = convert_col_to_tensor(processed_data.segment_ids).to(device)\n",
    "    mask        = convert_col_to_tensor(processed_data['mask']).to(device)\n",
    "    \n",
    "    data        = [tokens, segment_ids, mask]\n",
    "    \n",
    "    # Collect the target columns\n",
    "    if target_cols is not None:\n",
    "        targets     = torch.tensor(df[target_cols].values, dtype = torch.float32).to(device)\n",
    "        data.append(targets)\n",
    "\n",
    "    # Construct a Torch Dataset, then a DataLoader that random samples and batches\n",
    "    dataset     = TensorDataset(*data)\n",
    "    if debug:\n",
    "        dataset     = DataLoader(dataset, 1, shuffle = True)\n",
    "    else:\n",
    "        dataset     = DataLoader(dataset, 32, shuffle = True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Suppress warnings when tokenizing sentences longer than the allowed length of 512\n",
    "\n",
    "# Load the original data\n",
    "train_df_all = pd.read_csv('../input/google-quest-challenge/train.csv')\n",
    "test_df      = pd.read_csv('../input/google-quest-challenge/test.csv')\n",
    "\n",
    "if debug:\n",
    "    train_df_all = train_df_all.iloc[:10]\n",
    "\n",
    "# Create Train and Validation Splits\n",
    "train_df, valid_df = train_test_split(train_df_all, random_state = 42, train_size = 0.8)\n",
    "\n",
    "Timer.start()\n",
    "# Create PyTorch Datasets\n",
    "train = create_dataset(train_df, preprocess_BERT_base_uncased, target_cols)\n",
    "valid = create_dataset(valid_df, preprocess_BERT_base_uncased, target_cols)\n",
    "test  = create_dataset( test_df, preprocess_BERT_base_uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.57 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "Timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the BERT Model with a head\n",
    "class BERT(Module):\n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_embed = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "        self.classifier = Sequential(\n",
    "            Linear(768 * 2, 2048),\n",
    "            Dropout(dropout)     ,\n",
    "            ELU()                ,\n",
    "            Linear(2048   , 2048),\n",
    "            Dropout(dropout)     ,\n",
    "            ELU()                ,\n",
    "            Linear(2048   ,   30),\n",
    "        )\n",
    "        \n",
    "    def forward(self, tokens, segment_ids, mask):\n",
    "        # Apply the main BERT\n",
    "        sequence_output, pooled_output = self.bert_embed(tokens, segment_ids, mask)\n",
    "        \n",
    "        # Average the Pooled Outputs, taking into account the mask\n",
    "        mask_expanded = torch.unsqueeze(mask, dim = -1)\n",
    "        seq_reduced   = torch.sum(sequence_output * mask_expanded, dim = 1)\n",
    "        mask_size     = torch.sum(mask, dim = 1)\n",
    "        seq_reduced   = seq_reduced / mask_size\n",
    "        \n",
    "        # Concatenate the pooled and seq(uential)_reduced tensors\n",
    "        signal        = torch.cat([pooled_output, seq_reduced], dim = 1)\n",
    "        \n",
    "        # Run the forward classifier. Output: logits (for cross-entropy loss)\n",
    "        signal        = self.classifier(signal)\n",
    "        \n",
    "        return signal\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, model_name, \n",
    "          optimizer, loss_fn, \n",
    "          train_data, val_data, \n",
    "          epochs = 30, \n",
    "          early_stopping = 2,\n",
    "          restore_best_model = False\n",
    "         ):\n",
    "    \n",
    "    best_val_spearman = None\n",
    "    bast_val_epoch    = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        ############################################\n",
    "        #                Training                  #\n",
    "        ############################################\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}')\n",
    "        \n",
    "        # Set up a progress bar, training loss accumulator, mini batch counter\n",
    "        bar = ProgressBar(len(train_data))\n",
    "        bar.start()\n",
    "        train_loss_total = 0.0\n",
    "        mini_batch       = 0\n",
    "        \n",
    "        model.train()\n",
    "        for batch in train_data:\n",
    "            \n",
    "            # Do a gradient descent step\n",
    "            tokens, segment_ids, mask, target = batch\n",
    "            optimizer.zero_grad()\n",
    "            output = model(tokens, segment_ids, mask)\n",
    "            loss   = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute metrics and display progress bar\n",
    "            train_loss_batch   = loss.data.item()\n",
    "            train_loss_total  += train_loss_batch\n",
    "            \n",
    "            mini_batch        += 1\n",
    "            train_loss_running = train_loss_total / mini_batch\n",
    "            bar.update(mini_batch, {'train_loss' : train_loss_running})\n",
    "                \n",
    "        ############################################\n",
    "        #                Validation                #\n",
    "        ############################################\n",
    "\n",
    "        \n",
    "        # Compute validation loss and metrics\n",
    "        val_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets     = []\n",
    "        model.eval()\n",
    "        for batch in val_data:\n",
    "            \n",
    "            # Compute the batch loss\n",
    "            tokens, segment_ids, mask, target = batch\n",
    "            output   = model(tokens, segment_ids, mask)\n",
    "            loss     = loss_fn(output, target)\n",
    "            val_loss += loss / len(val_data)\n",
    "            \n",
    "            # Accumulate the predictions and targets\n",
    "            processed_outputs = torch.sigmoid(output).cpu()\n",
    "            all_predictions.append(processed_outputs)\n",
    "            all_targets.append(target.cpu())\n",
    "            \n",
    "        all_predictions = torch.stack(all_predictions)\n",
    "        all_targets     = torch.stack(all_targets)\n",
    "        \n",
    "        # Calculate Spearman correlation coefficient\n",
    "        spearman_coef = 0.0\n",
    "        NUM_TARGETS   = all_targets.shape[-1]\n",
    "        \n",
    "        all_targets     =     all_targets.detach().cpu().numpy().squeeze()\n",
    "        all_predictions = all_predictions.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        for i in range(NUM_TARGETS):\n",
    "            spearman_coef += spearmanr(all_targets[:, i], all_predictions[:, i]).correlation / NUM_TARGETS\n",
    "        \n",
    "        bar.update(mini_batch, {'train_loss'     : train_loss_running,\n",
    "                                'val_loss'       : val_loss, \n",
    "                                'val_spearman'   : spearman_coef\n",
    "                               })\n",
    "        \n",
    "        ############################################\n",
    "        #                Callbacks                 #\n",
    "        ############################################\n",
    "\n",
    "        \n",
    "        # Checkpoint saving\n",
    "        if best_val_spearman is None or best_val_spearman < spearman_coef:\n",
    "            # Save the new best model, overwriting the old one\n",
    "            torch.save(model, f'./checkpoints/{model_name}.pt')\n",
    "            best_val_epoch = epoch\n",
    "            \n",
    "        # Early Stopping\n",
    "        elif epoch >= best_val_epoch + early_stopping:\n",
    "            if restore_best_model:\n",
    "                model = torch.load(f'./checkpoints/{model_name}.pt')\n",
    "            break\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(output_batch, target_batch):\n",
    "    return binary_cross_entropy(torch.sigmoid(output_batch), target_batch, reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/rcharan/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "model     = BERT(0.1)\n",
    "optimizer = Adam(model.classifier.parameters())\n",
    "loss_fn   = cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, predictions = train_loop(model, 'bert_base_uncased_1', optimizer, loss_fn, train, valid, restore_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer.start()\n",
    "# writer = SummaryWriter()\n",
    "# writer.add_graph(model, next(iter(train))[:3])\n",
    "# Timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
