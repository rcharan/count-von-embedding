{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utilities/')\n",
    "from utilities import Timer, lmap\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect gpu availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: map returning a list\n",
    "def lmap(func, iterable):\n",
    "    return list(map(func, iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sc = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the Data\n",
    "### 1a. Preprocess/Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def preprocess_BERT(encoder, q_row):\n",
    "    '''Preprocess dataframe row for BERT\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder: callable, takes in text, returns encoded tokens, \n",
    "             should be provided by pre-trained model\n",
    "    \n",
    "    q_row  : dataframe row containing columns for question_title, \n",
    "             question_body, and answer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas Series of entries, each entry a list of length 512.\n",
    "    Entries: tokens, a mask, and positional embeddings.\n",
    "    '''\n",
    "    \n",
    "    # Set max length allowed by BERT model\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Get question title, body, and answer from dataframe row\n",
    "    question = q_row.question_title + q_row.question_body\n",
    "    answer   = q_row.answer\n",
    "    \n",
    "    # Encode question and answer without [CLS] and [SEP]\n",
    "    question_tok = encoder(question, add_special_tokens = False)\n",
    "    answer_tok   = encoder(answer, add_special_tokens = False)\n",
    "\n",
    "    # Truncate tokens to length MAX_LENGTH - 3 to account for special tokens\n",
    "    while len(question_tok + answer_tok) > (MAX_LENGTH - 3):\n",
    "        \n",
    "        # Pick the longest list, then pop last item in list\n",
    "        # Default to shortening answer if there is a tie\n",
    "        array_to_pop = max([answer_tok, question_tok], key = len)\n",
    "        array_to_pop.pop()\n",
    "    \n",
    "    # Get encodings for [CLS] and [SEP]\n",
    "    cls_token_encoded = encoder(['[CLS]'], add_special_tokens = False)\n",
    "    sep_token_encoded = encoder(['[SEP]'], add_special_tokens = False)\n",
    "    \n",
    "    # Combine question, answer, and special tokens\n",
    "    content_tok = cls_token_encoded + question_tok + \\\n",
    "                  sep_token_encoded + answer_tok   + \\\n",
    "                  sep_token_encoded\n",
    "    \n",
    "    # Create padding\n",
    "    padding_len = MAX_LENGTH - len(content_tok)\n",
    "    padding     = [0] * padding_len\n",
    "    \n",
    "    # Add padding\n",
    "    final_tok   = content_tok + padding\n",
    "    \n",
    "    # Compute segment_ids\n",
    "    segment_ids = [0] * (len(question_tok) + 2) + \\\n",
    "                  [1] * (len(answer_tok)   + 1) + \\\n",
    "                  padding\n",
    "    \n",
    "    # Compute the mask\n",
    "    mask        = [1] * len(content_tok) + padding\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tokens'      : final_tok,\n",
    "        'segment_ids' : segment_ids,\n",
    "        'mask'        : mask\n",
    "    })\n",
    "\n",
    "# Load in tokenizer for BERT base uncased\n",
    "BERT_base_uncased_tokenizer  = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased') \n",
    "\n",
    "# Curry preprocess function and partially apply it\n",
    "preprocess_BERT_base_uncased = functools.partial(preprocess_BERT, \n",
    "                                                 BERT_base_uncased_tokenizer.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Create a PyTorch Dataset from the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice', \n",
    "               'question_type_compare', 'question_type_consequence', \n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful', \n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, preprocessor, target_cols = None):\n",
    "    '''Create a dataset from a pandas dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df: Pandas dataframe with text columns available for the preprocessor \n",
    "        and containing the target columns\n",
    "        \n",
    "    preprocessor: callable taking a row of a dataframe and returning \n",
    "                  a Series containing the inputs as lists in each entry\n",
    "                  \n",
    "    target_cols: list of column names to use as the target.\n",
    "    If None, no labels are included\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    PyTorch Dataset (batched)\n",
    "    \n",
    "    '''\n",
    "    # Process the input data into a dataframe with 3 columns\n",
    "    processed_data = df.apply(preprocessor, axis = 'columns')\n",
    "\n",
    "    # Convert each of those three columns into a tensor\n",
    "    def convert_col_to_tensor(col):\n",
    "        # Convert each list entry to a tensor. Then stack them into one large tensor\n",
    "        col = lmap(lambda list_ : torch.tensor(list_, dtype = torch.long), col.tolist())\n",
    "        return torch.stack(col)\n",
    "\n",
    "    tokens      = convert_col_to_tensor(processed_data.tokens).to(device)\n",
    "    segment_ids = convert_col_to_tensor(processed_data.segment_ids).to(device)\n",
    "    mask        = convert_col_to_tensor(processed_data['mask']).to(device)\n",
    "    \n",
    "    data        = [tokens, segment_ids, mask]\n",
    "    \n",
    "    # Collect the target columns\n",
    "    if target_cols is not None:\n",
    "        targets     = torch.tensor(df[target_cols].values, dtype = torch.float32).to(device)\n",
    "        data.append(targets)\n",
    "\n",
    "    # Construct a Torch Dataset, then a DataLoader that random samples and batches\n",
    "    dataset     = TensorDataset(*data)\n",
    "    dataset     = DataLoader(dataset, 32, shuffle = True)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Suppress warnings when tokenizing sentences longer than the allowed length of 512\n",
    "\n",
    "# Load the original data\n",
    "train_df_all = pd.read_csv('../input/google-quest-challenge/train.csv')\n",
    "test_df      = pd.read_csv('../input/google-quest-challenge/test.csv')\n",
    "\n",
    "# Create Train and Validation Splits\n",
    "train_df, valid_df = train_test_split(train_df_all, random_state = 42, train_size = 0.8)\n",
    "\n",
    "Timer.start()\n",
    "# Create PyTorch Datasets\n",
    "train = create_dataset(train_df, preprocess_BERT_base_uncased, target_cols)\n",
    "valid = create_dataset(valid_df, preprocess_BERT_base_uncased, target_cols)\n",
    "test  = create_dataset( test_df, preprocess_BERT_base_uncased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
