{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports, setup, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch.cuda as cutorch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import Module, Linear, Dropout, ELU, Sequential, Sigmoid\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torch.optim import Adam\n",
    "from scipy.stats import spearmanr\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utilities/')\n",
    "from utilities import Timer, ProgressBar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect gpu availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: map returning a list\n",
    "def lmap(func, iterable):\n",
    "    return list(map(func, iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    BATCH_SIZE = 1\n",
    "else:\n",
    "    BATCH_SIZE = 12 # Max our 15GB-memory GPU could handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Memory Management\n",
    "def print_memory_usage():\n",
    "    for i in range(cutorch.device_count()):\n",
    "        print(f'GPU {i}     : {torch.cuda.memory_allocated(device=0):,d} Bytes')\n",
    "\n",
    "def print_max_memory_usage():\n",
    "    for i in range(cutorch.device_count()):\n",
    "        print(f'GPU {i} Peak: {torch.cuda.max_memory_allocated(device=0):,d} Bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the Data\n",
    "### 1a. Preprocess/Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def preprocess_BERT(encoder, q_row):\n",
    "    '''Preprocess dataframe row for BERT\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder: callable, takes in text, returns encoded tokens, \n",
    "             should be provided by pre-trained model\n",
    "    \n",
    "    q_row  : dataframe row containing columns for question_title, \n",
    "             question_body, and answer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas Series of entries, each entry a list of length 512.\n",
    "    Entries: tokens, a mask, and positional embeddings.\n",
    "    '''\n",
    "    \n",
    "    # Set max length allowed by BERT model\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Get question title, body, and answer from dataframe row\n",
    "    question = q_row.question_title + q_row.question_body\n",
    "    answer   = q_row.answer\n",
    "    \n",
    "    # Encode question and answer without [CLS] and [SEP]\n",
    "    question_tok = encoder(question, add_special_tokens = False)\n",
    "    answer_tok   = encoder(answer, add_special_tokens = False)\n",
    "\n",
    "    # Truncate tokens to length MAX_LENGTH - 3 to account for special tokens\n",
    "    while len(question_tok + answer_tok) > (MAX_LENGTH - 3):\n",
    "        \n",
    "        # Pick the longest list, then pop last item in list\n",
    "        # Default to shortening answer if there is a tie\n",
    "        array_to_pop = max([answer_tok, question_tok], key = len)\n",
    "        array_to_pop.pop()\n",
    "    \n",
    "    # Get encodings for [CLS] and [SEP]\n",
    "    cls_token_encoded = encoder(['[CLS]'], add_special_tokens = False)\n",
    "    sep_token_encoded = encoder(['[SEP]'], add_special_tokens = False)\n",
    "    \n",
    "    # Combine question, answer, and special tokens\n",
    "    content_tok = cls_token_encoded + question_tok + \\\n",
    "                  sep_token_encoded + answer_tok   + \\\n",
    "                  sep_token_encoded\n",
    "    \n",
    "    # Create padding\n",
    "    padding_len = MAX_LENGTH - len(content_tok)\n",
    "    padding     = [0] * padding_len\n",
    "    \n",
    "    # Add padding\n",
    "    final_tok   = content_tok + padding\n",
    "    \n",
    "    # Compute segment_ids\n",
    "    segment_ids = [0] * (len(question_tok) + 2) + \\\n",
    "                  [1] * (len(answer_tok)   + 1) + \\\n",
    "                  padding\n",
    "    \n",
    "    # Compute the mask\n",
    "    mask        = [1] * len(content_tok) + padding\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tokens'      : final_tok,\n",
    "        'segment_ids' : segment_ids,\n",
    "        'mask'        : mask\n",
    "    })\n",
    "\n",
    "# Load in tokenizer for BERT base uncased\n",
    "BERT_base_uncased_tokenizer  = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased') \n",
    "\n",
    "# Curry preprocess function and partially apply it\n",
    "preprocess_BERT_base_uncased = functools.partial(preprocess_BERT, \n",
    "                                                 BERT_base_uncased_tokenizer.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Create a PyTorch Dataset from the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice', \n",
    "               'question_type_compare', 'question_type_consequence', \n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful', \n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, preprocessor, target_cols = None):\n",
    "    '''Create a dataset from a pandas dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df: Pandas dataframe with text columns available for the preprocessor \n",
    "        and containing the target columns\n",
    "        \n",
    "    preprocessor: callable taking a row of a dataframe and returning \n",
    "                  a Series containing the inputs as lists in each entry\n",
    "                  \n",
    "    target_cols: list of column names to use as the target.\n",
    "    If None, no labels are included\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    PyTorch Dataset (batched)\n",
    "    \n",
    "    '''\n",
    "    # Process the input data into a dataframe with 3 columns\n",
    "    processed_data = df.apply(preprocessor, axis = 'columns')\n",
    "\n",
    "    # Convert each of those three columns into a tensor\n",
    "    def convert_col_to_tensor(col):\n",
    "        # Convert each list entry to a tensor. Then stack them into one large tensor\n",
    "        col = lmap(lambda list_ : torch.tensor(list_, dtype = torch.long), col.tolist())\n",
    "        return torch.stack(col)\n",
    "\n",
    "    tokens      = convert_col_to_tensor(processed_data.tokens).to(device)\n",
    "    segment_ids = convert_col_to_tensor(processed_data.segment_ids).to(device)\n",
    "    mask        = convert_col_to_tensor(processed_data['mask']).to(device)\n",
    "    \n",
    "    data        = [tokens, segment_ids, mask]\n",
    "    \n",
    "    # Collect the target columns\n",
    "    if target_cols is not None:\n",
    "        targets     = torch.tensor(df[target_cols].values, dtype = torch.float32).to(device)\n",
    "        data.append(targets)\n",
    "\n",
    "    # Construct a Torch Dataset, then a DataLoader that random samples and batches\n",
    "    dataset     = TensorDataset(*data)\n",
    "    dataset     = DataLoader(dataset, BATCH_SIZE, shuffle = True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Suppress warnings when tokenizing sentences longer than the allowed length of 512\n",
    "\n",
    "# Load the original data\n",
    "train_df_all = pd.read_csv('../input/google-quest-challenge/train.csv')\n",
    "test_df      = pd.read_csv('../input/google-quest-challenge/test.csv')\n",
    "\n",
    "if debug:\n",
    "    train_df_all = train_df_all.iloc[:1000]\n",
    "\n",
    "# Create Train and Validation Splits\n",
    "train_df, valid_df = train_test_split(train_df_all, random_state = 42, train_size = 0.8)\n",
    "\n",
    "Timer.start()\n",
    "# Create PyTorch Datasets\n",
    "train = create_dataset(train_df, preprocess_BERT_base_uncased, target_cols)\n",
    "valid = create_dataset(valid_df, preprocess_BERT_base_uncased, target_cols)\n",
    "test  = create_dataset( test_df, preprocess_BERT_base_uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.680 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "Timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the BERT Model with a head\n",
    "class BERT(Module):\n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_embed = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "        self.classifier = Sequential(\n",
    "            Linear(768 * 2, 2048),\n",
    "            Dropout(dropout)     ,\n",
    "            ELU()                ,\n",
    "            Linear(2048   , 2048),\n",
    "            Dropout(dropout)     ,\n",
    "            ELU()                ,\n",
    "            Linear(2048   , 2048),\n",
    "            Dropout(dropout)     ,\n",
    "            ELU()                ,\n",
    "            Linear(2048   , 2048),\n",
    "            Dropout(dropout)     ,\n",
    "            ELU()                ,\n",
    "            Linear(2048   ,   30),\n",
    "        )\n",
    "        \n",
    "    def forward(self, tokens, segment_ids, mask):\n",
    "        # Apply the main BERT\n",
    "        sequence_output, pooled_output = self.bert_embed(tokens, segment_ids, mask)\n",
    "        \n",
    "        # Average the Pooled Outputs, taking into account the mask\n",
    "        mask_expanded = torch.unsqueeze(mask, dim = -1)\n",
    "        seq_reduced   = torch.sum(sequence_output * mask_expanded, dim = 1)\n",
    "        mask_size     = torch.sum(mask, dim = 1, keepdim = True)\n",
    "        seq_reduced   = seq_reduced / mask_size\n",
    "        \n",
    "        # Concatenate the pooled and seq(uential)_reduced tensors\n",
    "        signal        = torch.cat([pooled_output, seq_reduced], dim = 1)\n",
    "        \n",
    "        # Run the forward classifier. Output: logits (for cross-entropy loss)\n",
    "        signal        = self.classifier(signal)\n",
    "        \n",
    "        return signal\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearmanr_safe(targets, predictions):\n",
    "    # When the targets are all the same, suppress the warning\n",
    "    with warnings.catch_warnings(record=True):\n",
    "        feature_spearman = spearmanr(targets, predictions).correlation\n",
    "    \n",
    "    if np.isnan(feature_spearman):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return feature_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, model_name, \n",
    "          optimizer, loss_fn, \n",
    "          train_data, val_data, \n",
    "          epochs = 30, \n",
    "          early_stopping = 2,\n",
    "          restore_best_model = False\n",
    "         ):\n",
    "    \n",
    "    best_val_spearman = None\n",
    "    bast_val_epoch    = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        ############################################\n",
    "        #                Training                  #\n",
    "        ############################################\n",
    "\n",
    "        \n",
    "        print(f'Epoch {epoch+1}')\n",
    "        \n",
    "        # Set up a progress bar, training loss accumulator, mini batch counter\n",
    "        bar = ProgressBar(len(train_data), bar_width = 30)\n",
    "        bar.start()\n",
    "        train_loss_total = 0.0\n",
    "        mini_batch       = 0\n",
    "        \n",
    "        model.train()\n",
    "        for batch in train_data:\n",
    "            \n",
    "            # Do a gradient descent step\n",
    "            tokens, segment_ids, mask, target = batch\n",
    "            optimizer.zero_grad()\n",
    "            output = model(tokens, segment_ids, mask)\n",
    "            loss   = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute metrics and display progress bar\n",
    "            train_loss_batch   = loss.data.item()\n",
    "            train_loss_total  += train_loss_batch\n",
    "            \n",
    "            mini_batch        += 1\n",
    "            train_loss_running = train_loss_total / mini_batch\n",
    "            bar.update(mini_batch, {'train_loss' : train_loss_running})\n",
    "            \n",
    "        del output\n",
    "        del loss\n",
    "        # print('Memory Usage\\n-------------------------------')\n",
    "        # print_max_memory_usage()\n",
    "        # print_memory_usage()\n",
    "                \n",
    "        ############################################\n",
    "        #                Validation                #\n",
    "        ############################################\n",
    "\n",
    "        \n",
    "        # Compute validation loss and metrics\n",
    "        val_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets     = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_data:\n",
    "\n",
    "                # Compute the batch loss\n",
    "                tokens, segment_ids, mask, target = batch\n",
    "                output   = model(tokens, segment_ids, mask)\n",
    "\n",
    "                loss     = loss_fn(output, target).item()\n",
    "                val_loss += loss / len(val_data)\n",
    "\n",
    "                # Accumulate the predictions and targets\n",
    "                processed_outputs = torch.sigmoid(output).cpu()\n",
    "                all_predictions.append(processed_outputs)\n",
    "                all_targets.append(target.cpu())\n",
    "            \n",
    "            \n",
    "            \n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        all_targets     = torch.cat(all_targets)\n",
    "        \n",
    "        # Calculate Spearman correlation coefficient\n",
    "        spearman_coef = 0.0\n",
    "        NUM_TARGETS   = all_targets.shape[-1]\n",
    "        \n",
    "        all_targets     =     all_targets.cpu().numpy().squeeze()\n",
    "        all_predictions = all_predictions.cpu().numpy().squeeze()\n",
    "                \n",
    "        for i in range(NUM_TARGETS):\n",
    "            feature_spearman = spearmanr_safe(all_targets[:, i], all_predictions[:, i])\n",
    "            spearman_coef   += feature_spearman / NUM_TARGETS\n",
    "        \n",
    "        bar.update(mini_batch, {'train_loss'     : train_loss_running,\n",
    "                                'val_loss'       : val_loss, \n",
    "                                'val_spearman'   : spearman_coef\n",
    "                               })\n",
    "        \n",
    "        ############################################\n",
    "        #                Callbacks                 #\n",
    "        ############################################\n",
    "\n",
    "        \n",
    "        # Checkpoint saving\n",
    "        if best_val_spearman is None or best_val_spearman < spearman_coef:\n",
    "            # Save the new best model, overwriting the old one\n",
    "            # Suppress warnings due to inability to acquire source for BERT\n",
    "            with warnings.catch_warnings(record=True):\n",
    "                torch.save(model, f'./checkpoints/{model_name}.pt')\n",
    "            best_val_epoch    = epoch\n",
    "            best_val_spearman = spearman_coef\n",
    "            \n",
    "        # Early Stopping\n",
    "        elif epoch >= best_val_epoch + early_stopping:\n",
    "            if restore_best_model:\n",
    "                model = torch.load(f'./checkpoints/{model_name}.pt')\n",
    "            break\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(output_batch, target_batch):\n",
    "    return binary_cross_entropy(torch.sigmoid(output_batch), target_batch, reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model     = BERT(0.1).to(device)\n",
    "# optimizer = Adam(model.classifier.parameters())\n",
    "# loss_fn   = cross_entropy_loss\n",
    "\n",
    "# model = train_loop(model, 'bert_base_uncased_2', optimizer, loss_fn,\n",
    "#                    train, valid, \n",
    "#                    restore_best_model=True,\n",
    "#                    early_stopping=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_memory():\n",
    "    print('Memory Usage\\n-------------------------------')\n",
    "    print_max_memory_usage()\n",
    "    print_memory_usage()\n",
    "    print('Dumping CUDA Cache')\n",
    "    try:\n",
    "        del model\n",
    "        del loss_fn\n",
    "        del optimizer\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dump_memory()\n",
    "model_name = 'bert_base_uncased_1'\n",
    "model     = torch.load(f'./checkpoints/{model_name}.pt').to(device)\n",
    "loss_fn   = cross_entropy_loss\n",
    "optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "model     = train_loop(model, f'{model_name}-fine_tuned', optimizer, loss_fn, train, valid, restore_best_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[==============================] 406/406 0.75s per loop train_loss : 0.40 val_loss : 0.40 val_spearman : 0.30          Epoch 2\n",
      "[==============================] 406/406 0.75s per loop train_loss : 0.39 val_loss : 0.39 val_spearman : 0.31          Epoch 3\n",
      "[==============================] 406/406 0.75s per loop train_loss : 0.38 val_loss : 0.39 val_spearman : 0.32          Epoch 4\n",
      "[==============================] 406/406 0.75s per loop train_loss : 0.37 val_loss : 0.40 val_spearman : 0.32          Epoch 5\n",
      "[==============================] 406/406 0.75s per loop train_loss : 0.36 val_loss : 0.40 val_spearman : 0.31          Epoch 6\n",
      "[=======>---------------------] 98/406 ETA: 3:33 train_loss : 0.36          "
     ]
    }
   ],
   "source": [
    "dump_memory()\n",
    "model_name = 'bert_base_uncased_1'\n",
    "model     = torch.load(f'./checkpoints/{model_name}.pt').to(device)\n",
    "loss_fn   = cross_entropy_loss\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "model     = train_loop(model, f'{model_name}-fine_tuned-fast', optimizer, loss_fn, train, valid, restore_best_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def view_memory()\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (has_attr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                print(type(obj), obj.size())\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timer.start()\n",
    "# writer = SummaryWriter()\n",
    "# writer.add_graph(model, next(iter(train))[:3])\n",
    "# Timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
