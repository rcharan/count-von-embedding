{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports, setup, utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import Module, Linear, Dropout, ELU, Sequential, Sigmoid\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utilities/')\n",
    "from utilities import Timer\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect gpu availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function: map returning a list\n",
    "def lmap(func, iterable):\n",
    "    return list(map(func, iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare the Data\n",
    "### 1a. Preprocess/Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def preprocess_BERT(encoder, q_row):\n",
    "    '''Preprocess dataframe row for BERT\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder: callable, takes in text, returns encoded tokens, \n",
    "             should be provided by pre-trained model\n",
    "    \n",
    "    q_row  : dataframe row containing columns for question_title, \n",
    "             question_body, and answer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas Series of entries, each entry a list of length 512.\n",
    "    Entries: tokens, a mask, and positional embeddings.\n",
    "    '''\n",
    "    \n",
    "    # Set max length allowed by BERT model\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Get question title, body, and answer from dataframe row\n",
    "    question = q_row.question_title + q_row.question_body\n",
    "    answer   = q_row.answer\n",
    "    \n",
    "    # Encode question and answer without [CLS] and [SEP]\n",
    "    question_tok = encoder(question, add_special_tokens = False)\n",
    "    answer_tok   = encoder(answer, add_special_tokens = False)\n",
    "\n",
    "    # Truncate tokens to length MAX_LENGTH - 3 to account for special tokens\n",
    "    while len(question_tok + answer_tok) > (MAX_LENGTH - 3):\n",
    "        \n",
    "        # Pick the longest list, then pop last item in list\n",
    "        # Default to shortening answer if there is a tie\n",
    "        array_to_pop = max([answer_tok, question_tok], key = len)\n",
    "        array_to_pop.pop()\n",
    "    \n",
    "    # Get encodings for [CLS] and [SEP]\n",
    "    cls_token_encoded = encoder(['[CLS]'], add_special_tokens = False)\n",
    "    sep_token_encoded = encoder(['[SEP]'], add_special_tokens = False)\n",
    "    \n",
    "    # Combine question, answer, and special tokens\n",
    "    content_tok = cls_token_encoded + question_tok + \\\n",
    "                  sep_token_encoded + answer_tok   + \\\n",
    "                  sep_token_encoded\n",
    "    \n",
    "    # Create padding\n",
    "    padding_len = MAX_LENGTH - len(content_tok)\n",
    "    padding     = [0] * padding_len\n",
    "    \n",
    "    # Add padding\n",
    "    final_tok   = content_tok + padding\n",
    "    \n",
    "    # Compute segment_ids\n",
    "    segment_ids = [0] * (len(question_tok) + 2) + \\\n",
    "                  [1] * (len(answer_tok)   + 1) + \\\n",
    "                  padding\n",
    "    \n",
    "    # Compute the mask\n",
    "    mask        = [1] * len(content_tok) + padding\n",
    "    \n",
    "    return pd.Series({\n",
    "        'tokens'      : final_tok,\n",
    "        'segment_ids' : segment_ids,\n",
    "        'mask'        : mask\n",
    "    })\n",
    "\n",
    "# Load in tokenizer for BERT base uncased\n",
    "BERT_base_uncased_tokenizer  = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased') \n",
    "\n",
    "# Curry preprocess function and partially apply it\n",
    "preprocess_BERT_base_uncased = functools.partial(preprocess_BERT, \n",
    "                                                 BERT_base_uncased_tokenizer.encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Create a PyTorch Dataset from the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['question_asker_intent_understanding', 'question_body_critical', \n",
    "               'question_conversational', 'question_expect_short_answer', \n",
    "               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n",
    "               'question_interestingness_others', 'question_interestingness_self', \n",
    "               'question_multi_intent', 'question_not_really_a_question', \n",
    "               'question_opinion_seeking', 'question_type_choice', \n",
    "               'question_type_compare', 'question_type_consequence', \n",
    "               'question_type_definition', 'question_type_entity', \n",
    "               'question_type_instructions', 'question_type_procedure', \n",
    "               'question_type_reason_explanation', 'question_type_spelling', \n",
    "               'question_well_written', 'answer_helpful', \n",
    "               'answer_level_of_information', 'answer_plausible', \n",
    "               'answer_relevance', 'answer_satisfaction', \n",
    "               'answer_type_instructions', 'answer_type_procedure', \n",
    "               'answer_type_reason_explanation', 'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, preprocessor, target_cols = None):\n",
    "    '''Create a dataset from a pandas dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df: Pandas dataframe with text columns available for the preprocessor \n",
    "        and containing the target columns\n",
    "        \n",
    "    preprocessor: callable taking a row of a dataframe and returning \n",
    "                  a Series containing the inputs as lists in each entry\n",
    "                  \n",
    "    target_cols: list of column names to use as the target.\n",
    "    If None, no labels are included\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    PyTorch Dataset (batched)\n",
    "    \n",
    "    '''\n",
    "    # Process the input data into a dataframe with 3 columns\n",
    "    processed_data = df.apply(preprocessor, axis = 'columns')\n",
    "\n",
    "    # Convert each of those three columns into a tensor\n",
    "    def convert_col_to_tensor(col):\n",
    "        # Convert each list entry to a tensor. Then stack them into one large tensor\n",
    "        col = lmap(lambda list_ : torch.tensor(list_, dtype = torch.long), col.tolist())\n",
    "        return torch.stack(col)\n",
    "\n",
    "    tokens      = convert_col_to_tensor(processed_data.tokens).to(device)\n",
    "    segment_ids = convert_col_to_tensor(processed_data.segment_ids).to(device)\n",
    "    mask        = convert_col_to_tensor(processed_data['mask']).to(device)\n",
    "    \n",
    "    data        = [tokens, segment_ids, mask]\n",
    "    \n",
    "    # Collect the target columns\n",
    "    if target_cols is not None:\n",
    "        targets     = torch.tensor(df[target_cols].values, dtype = torch.float32).to(device)\n",
    "        data.append(targets)\n",
    "\n",
    "    # Construct a Torch Dataset, then a DataLoader that random samples and batches\n",
    "    dataset     = TensorDataset(*data)\n",
    "    if debug:\n",
    "        dataset     = DataLoader(dataset, 1, shuffle = True)\n",
    "    else:\n",
    "        dataset     = DataLoader(dataset, 32, shuffle = True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Suppress warnings when tokenizing sentences longer than the allowed length of 512\n",
    "\n",
    "# Load the original data\n",
    "train_df_all = pd.read_csv('../input/google-quest-challenge/train.csv')\n",
    "test_df      = pd.read_csv('../input/google-quest-challenge/test.csv')\n",
    "\n",
    "if debug:\n",
    "    train_df_all = train_df_all.iloc[:100]\n",
    "\n",
    "# Create Train and Validation Splits\n",
    "train_df, valid_df = train_test_split(train_df_all, random_state = 42, train_size = 0.8)\n",
    "\n",
    "Timer.start()\n",
    "# Create PyTorch Datasets\n",
    "train = create_dataset(train_df, preprocess_BERT_base_uncased, target_cols)\n",
    "valid = create_dataset(valid_df, preprocess_BERT_base_uncased, target_cols)\n",
    "test  = create_dataset( test_df, preprocess_BERT_base_uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.167 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "Timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the BERT Model with a head\n",
    "class BERT(Module):\n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_embed = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')\n",
    "        self.classifier = Sequential(\n",
    "            Linear(768 * 2, 2048),\n",
    "            Dropout(dropout)     ,\n",
    "            ELU()                ,\n",
    "            Linear(2048   , 2048),\n",
    "            Dropout(dropout)     ,\n",
    "            ELU()                ,\n",
    "            Linear(2048   ,   30),\n",
    "        )\n",
    "        \n",
    "    def forward(self, tokens, segment_ids, mask):\n",
    "        # Apply the main BERT\n",
    "        sequence_output, pooled_output = self.bert_embed(tokens, segment_ids, mask)\n",
    "        \n",
    "        # Average the Pooled Outputs, taking into account the mask\n",
    "        mask_expanded = torch.unsqueeze(mask, dim = -1)\n",
    "        seq_reduced   = torch.sum(sequence_output * mask_expanded, dim = 1)\n",
    "        mask_size     = torch.sum(mask, dim = 1)\n",
    "        seq_reduced   = seq_reduced / mask_size\n",
    "        \n",
    "        # Concatenate the pooled and seq(uential)_reduced tensors\n",
    "        signal        = torch.cat([pooled_output, seq_reduced], dim = 1)\n",
    "        \n",
    "        # Run the forward classifier. Output: logits (for cross-entropy loss)\n",
    "        signal        = self.classifier(signal)\n",
    "        \n",
    "        return signal\n",
    "        \n",
    "    def predict(self, tokens, segment_ids, mask):\n",
    "        logits = self.forward(tokens, segment_ids, mask)\n",
    "        return Sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/rcharan/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "model = BERT(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data = next(iter(train))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0430, -0.0765, -0.0079,  0.0461, -0.0545, -0.1626, -0.0232, -0.0142,\n",
       "          0.0707, -0.0861,  0.0063, -0.0708,  0.0600,  0.0151, -0.0312, -0.0510,\n",
       "          0.1179, -0.0654,  0.1387, -0.0829,  0.0268, -0.0163, -0.0715, -0.0198,\n",
       "         -0.0059, -0.0385, -0.0794,  0.0588,  0.0741,  0.1672]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(*trial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.566 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# Timer.start()\n",
    "# writer = SummaryWriter()\n",
    "# writer.add_graph(model, next(iter(train))[:3])\n",
    "# Timer.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
